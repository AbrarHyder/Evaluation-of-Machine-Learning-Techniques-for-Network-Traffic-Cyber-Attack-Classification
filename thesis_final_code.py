# -*- coding: utf-8 -*-
"""Thesis final code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TJMwOeyDgWxAJTcySSWO9XnKuVq_iudT
"""

# prompt: write code to unzip a file

import zipfile

with zipfile.ZipFile("/content/Cybersecurity_attacks.csv.zip", 'r') as zip_ref:
    zip_ref.extractall("")

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

df = pd.read_csv('Cybersecurity_attacks.csv')
df.head()

df.shape

from datetime import datetime, timedelta

df[['Start time','End time']] = df['Time'].str.split('-',expand=True)
df.head()

df = df.drop(['.', 'Time'],axis=1)
df.head()

!pip install missingno

import missingno as msno

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))


msno.matrix(df, ax=ax1, sparkline=True, color=(0.2, 0.4, 0.6))
ax1.set_title("Missing Data Matrix", fontsize=14)


msno.bar(df, ax=ax2, color=(0.6, 0.3, 0.3))
ax2.set_title("Missing Data Bar Chart", fontsize=14)

# Display the plots
plt.tight_layout()
plt.show()

df["Attack subcategory"] = df["Attack subcategory"].fillna("Null")

df.isnull().sum()

df[df.duplicated()]

df[df.duplicated()].shape

df.shape

df = df.drop(df[df.duplicated()].index)
df.shape

invalid_SourcePort = (df['Source Port'] < 0) | (df['Source Port'] > 65535)
invalid_DestinationPort = (df['Destination Port'] < 0) | (df['Destination Port'] > 65535)
df[invalid_SourcePort | invalid_DestinationPort]

df = df[~(invalid_SourcePort | invalid_DestinationPort)].reset_index(drop=True)

df.shape

df['Protocol'].unique()

df['Attack category'].unique()

df['Protocol'] = df['Protocol'].str.upper().str.strip()
df['Attack category'] = df['Attack category'].str.upper().str.strip()
df['Attack category'] = df['Attack category'].str.strip().replace('BACKDOORS','BACKDOOR')

df.head()

df.tail()

print(df['Attack category'].value_counts())

print(df[pd.isnull(df['Attack Reference'])]['Attack category'].value_counts())

tp = pd.read_csv('TCP-ports.csv')
tp.head()

tp['Service'] = tp['Service'].str.upper()
tp.head()

merge_df = pd.merge(df, tp[['Port','Service']], left_on='Destination Port', right_on='Port', how='left')
merge_df.head()

merge_df['Attack category'].value_counts()

merge_df['Attack category'].value_counts()*100/merge_df['Attack category'].value_counts().sum()

import seaborn as sns

sns.set_style("whitegrid")


plt.figure(figsize=(22, 10))

sorted_data = merge_df['Attack category'].value_counts()
sns.barplot(
    x=sorted_data.index,
    y=sorted_data.values,
    palette="coolwarm"
)


plt.xlabel('Attack Category', fontsize=20, weight='bold')
plt.ylabel('Count', fontsize=20, weight='bold')
plt.title('Number of Attacks per Category', fontsize=24, weight='bold')


plt.xticks(rotation=45, ha='right', fontsize=16, weight='bold')
plt.yticks(fontsize=16, weight='bold')


for index, value in enumerate(sorted_data.values):
    plt.text(index, value + 0.5, str(value), ha='center', va='bottom', fontsize=16, weight='bold', color='black')


plt.grid(visible=True, which='both', axis='y', linestyle='--', linewidth=0.7)


plt.tight_layout()
plt.show()

merge_df['Start time'] = pd.to_datetime(merge_df['Start time'], unit='s')
merge_df['End time'] = pd.to_datetime(merge_df['End time'], unit='s')
merge_df['Duration'] = ((merge_df['End time'] - merge_df['Start time']).dt.seconds).astype(int)
merge_df.head()

merge_df.describe()

merge_df['Destination IP'].value_counts()

merge_df.head()

drop_columns = ['Attack Reference', 'Start time', 'End time', 'Source IP', 'Destination IP']
X = merge_df.drop(columns=drop_columns)

X.head()

from sklearn.preprocessing import LabelEncoder

features = ['Protocol', 'Source Port', 'Destination Port',  'Attack subcategory']
X = df[features].copy()
y = df['Attack category']

le_protocol = LabelEncoder()
X['Protocol'] = le_protocol.fit_transform(X['Protocol'])

le_attack_subcategory = LabelEncoder()
X['Attack subcategory'] = le_attack_subcategory.fit_transform(X['Attack subcategory'])

import joblib
joblib.dump(le_protocol, "protocol_encoder.joblib")
joblib.dump(le_attack_subcategory, "attack_subcategory_encoder.joblib")

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print("Training features shape:", X_train.shape)
print("Testing features shape:", X_test.shape)
print("Training labels shape:", y_train.shape)
print("Testing labels shape:", y_test.shape)

from sklearn.metrics import accuracy_score
from sklearn.neighbors import KNeighborsClassifier

knn_model = KNeighborsClassifier(n_neighbors=5)

knn_model.fit(X, y)

y_pred_knn = knn_model.predict(X_test)

accuracy = accuracy_score(y_test, y_pred_knn)
accuracy

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
print (print("Accuracy test set: ", accuracy_score(y_test, y_pred_knn)))
print (print("Precision test set: ", precision_score(y_test, y_pred_knn, average = 'micro' )))
print (print("Recall test set: ", recall_score(y_test, y_pred_knn, average = 'micro' )))
print (print("F-1 score test set: ", f1_score(y_test, y_pred_knn, average = 'micro' )))

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

# Compute the confusion matrix
cm = confusion_matrix(y_test, y_pred_knn)

# Set up the figure size (width, height) in inches
plt.figure(figsize=(16, 14))

# Display the confusion matrix
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=knn_model.classes_)
disp.plot(cmap="Blues", values_format="d", ax=plt.gca())

# Show the plot
plt.show()

import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score

def quick_confusion_matrix(y_test, y_pred_knn, knn_model):
    """
    Fast confusion matrix plot
    """
    plt.figure(figsize=(10, 8))
    cm = confusion_matrix(y_test, y_pred_knn)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=knn_model.classes_)
    disp.plot(cmap="Blues", values_format="d")
    plt.show()

def quick_k_accuracy(X_train, X_test, y_train, y_test, k_values=[1,3,5,7,9]):
    """
    Fast k vs accuracy plot with fewer k values
    """
    accuracies = []
    from sklearn.neighbors import KNeighborsClassifier

    for k in k_values:
        model = KNeighborsClassifier(n_neighbors=k)
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        accuracies.append(accuracy_score(y_test, y_pred))

    plt.figure(figsize=(8, 6))
    plt.plot(k_values, accuracies, 'bo-')
    plt.xlabel('k')
    plt.ylabel('Accuracy')
    plt.title('Accuracy vs k Value')
    plt.grid(True)
    plt.show()

# Example usage:

# Quick confusion matrix
quick_confusion_matrix(y_test, y_pred_knn, knn_model)

# Quick k-accuracy plot
quick_k_accuracy(X_train, X_test, y_train, y_test)

from sklearn.linear_model import LogisticRegression

clf = LogisticRegression(random_state=0).fit(X, y)

from sklearn.metrics import accuracy_score

y_pred_clf = clf.predict(X_test)
accuracy_clf = accuracy_score(y_test, y_pred_clf)
accuracy_clf

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
print (print("Accuracy test set: ", accuracy_score(y_test, y_pred_clf)))

print (print("Precision test set: ", precision_score(y_test, y_pred_clf, average = 'micro' )))
print (print("Recall test set: ", recall_score(y_test, y_pred_clf, average = 'micro' )))
print (print("F-1 score test set: ", f1_score(y_test, y_pred_clf, average = 'micro' )))

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

# Compute the confusion matrix
cm = confusion_matrix(y_test, y_pred_clf)

# Set the figure size
plt.figure(figsize=(16, 14))  # Adjust width and height as needed

# Display the confusion matrix
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=clf.classes_)
disp.plot(cmap="Blues", values_format="d", ax=plt.gca())

# Show the plot
plt.show()

import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import (accuracy_score, precision_score, recall_score,
                           f1_score, confusion_matrix, ConfusionMatrixDisplay,
                           roc_curve, auc, precision_recall_curve)

def plot_metrics_summary(y_test, y_pred_clf):
    """
    Print all metrics and create a bar plot
    """
    # Calculate metrics
    accuracy = accuracy_score(y_test, y_pred_clf)
    precision = precision_score(y_test, y_pred_clf, average='micro')
    recall = recall_score(y_test, y_pred_clf, average='micro')
    f1 = f1_score(y_test, y_pred_clf, average='micro')

    # Print metrics
    print(f"Accuracy test set: {accuracy:.3f}")
    print(f"Precision test set: {precision:.3f}")
    print(f"Recall test set: {recall:.3f}")
    print(f"F-1 score test set: {f1:.3f}")

    # Create bar plot
    metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']
    values = [accuracy, precision, recall, f1]

    plt.figure(figsize=(10, 6))
    bars = plt.bar(metrics, values)
    plt.ylim(0, 1)
    plt.title('Model Performance Metrics')

    # Add value labels on top of bars
    for bar in bars:
        height = bar.get_height()
        plt.text(bar.get_x() + bar.get_width()/2., height,
                f'{height:.3f}', ha='center', va='bottom')

    plt.show()

def plot_confusion_matrix(y_test, y_pred_clf, model):
    """
    Plot confusion matrix
    """
    plt.figure(figsize=(10, 8))
    cm = confusion_matrix(y_test, y_pred_clf)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)
    disp.plot(cmap="Blues", values_format="d")
    plt.title('Confusion Matrix')
    plt.show()

def plot_roc_curve(y_test, y_pred_proba, n_classes):
    """
    Plot ROC curve (works for binary and multiclass)
    """
    plt.figure(figsize=(10, 6))

    if n_classes == 2:
        # Binary classification
        fpr, tpr, _ = roc_curve(y_test, y_pred_proba[:, 1])
        roc_auc = auc(fpr, tpr)

        plt.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc:.2f})')
        plt.plot([0, 1], [0, 1], 'k--')  # diagonal line

    else:
        # Multiclass - One vs Rest
        for i in range(n_classes):
            fpr, tpr, _ = roc_curve(y_test == i, y_pred_proba[:, i])
            roc_auc = auc(fpr, tpr)
            plt.plot(fpr, tpr, label=f'Class {i} (AUC = {roc_auc:.2f})')

        plt.plot([0, 1], [0, 1], 'k--')

    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver Operating Characteristic (ROC) Curve')
    plt.legend(loc="lower right")
    plt.grid(True)
    plt.show()

def plot_feature_importance(model, feature_names=None):
    """
    Plot feature importance based on logistic regression coefficients
    """
    if feature_names is None:
        feature_names = [f'Feature {i}' for i in range(len(model.coef_[0]))]

    # Get absolute coefficients for feature importance
    importance = np.abs(model.coef_[0])

    # Sort features by importance
    sorted_idx = np.argsort(importance)
    pos = np.arange(sorted_idx.shape[0]) + .5

    plt.figure(figsize=(10, 6))
    plt.barh(pos, importance[sorted_idx])
    plt.yticks(pos, np.array(feature_names)[sorted_idx])
    plt.xlabel('Absolute Coefficient Value')
    plt.title('Feature Importance')
    plt.tight_layout()
    plt.show()

# Example usage:

# For metrics summary and confusion matrix
plot_metrics_summary(y_test, y_pred_clf)
plot_confusion_matrix(y_test, y_pred_clf, clf)

# For ROC curve (assuming you have probability predictions)
y_pred_proba = clf.predict_proba(X_test)
n_classes = len(clf.classes_)
plot_roc_curve(y_test, y_pred_proba, n_classes)

# For feature importance (optional: provide feature names)
feature_names = ['Protocol', 'Source Port', 'Destination Port',  'Attack subcategory']  # your feature names
plot_feature_importance(clf, feature_names)

from sklearn.linear_model import SGDClassifier

sgd_classifier = SGDClassifier(loss='log_loss', max_iter=1000, random_state=42) #max_iter is epochs

sgd_classifier.fit(X, y)

from sklearn.metrics import accuracy_score

y_pred = sgd_classifier.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
accuracy

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
print (print("Accuracy test set: ", accuracy_score(y_test, y_pred)))

print (print("Precision test set: ", precision_score(y_test, y_pred, average = 'micro' )))
print (print("Recall test set: ", recall_score(y_test, y_pred, average = 'micro' )))
print (print("F-1 score test set: ", f1_score(y_test, y_pred, average = 'micro' )))

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

# Compute the confusion matrix
cm = confusion_matrix(y_test, y_pred)

# Set the figure size
plt.figure(figsize=(16, 14))  # Adjust width and height as needed

# Display the confusion matrix
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=sgd_classifier.classes_)
disp.plot(cmap="Blues", values_format="d", ax=plt.gca())

# Show the plot
plt.show()

import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import (accuracy_score, precision_score, recall_score,
                           f1_score, confusion_matrix, ConfusionMatrixDisplay,
                           roc_curve, auc)

def plot_sgd_metrics(y_test, y_pred_sgd):
    """
    Plot and print all metrics for SGD Classifier
    """
    # Calculate metrics
    accuracy = accuracy_score(y_test, y_pred_sgd)
    precision = precision_score(y_test, y_pred_sgd, average='micro')
    recall = recall_score(y_test, y_pred_sgd, average='micro')
    f1 = f1_score(y_test, y_pred_sgd, average='micro')

    # Print metrics
    print(f"Accuracy: {accuracy:.3f}")
    print(f"Precision: {precision:.3f}")
    print(f"Recall: {recall:.3f}")
    print(f"F1-Score: {f1:.3f}")

    # Create bar plot
    metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']
    values = [accuracy, precision, recall, f1]

    plt.figure(figsize=(10, 6))
    bars = plt.bar(metrics, values)
    plt.ylim(0, 1)
    plt.title('SGD Classifier Performance Metrics')

    # Add value labels on bars
    for bar in bars:
        height = bar.get_height()
        plt.text(bar.get_x() + bar.get_width()/2., height,
                f'{height:.3f}', ha='center', va='bottom')
    plt.show()

def plot_sgd_confusion_matrix(y_test, y_pred_sgd, sgd_model):
    """
    Plot confusion matrix for SGD Classifier
    """
    plt.figure(figsize=(10, 8))
    cm = confusion_matrix(y_test, y_pred_sgd)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=sgd_model.classes_)
    disp.plot(cmap="Blues", values_format="d")
    plt.title('SGD Classifier Confusion Matrix')
    plt.show()

def plot_sgd_learning_curve(sgd_model, X_train, y_train, n_iterations=100):
    """
    Plot learning curve during SGD training
    Requires partial_fit to be used during training
    """
    train_scores = []

    # Train the model incrementally and collect scores
    for i in range(n_iterations):
        sgd_model.partial_fit(X_train, y_train, classes=np.unique(y_train))
        score = sgd_model.score(X_train, y_train)
        train_scores.append(score)

    plt.figure(figsize=(10, 6))
    plt.plot(range(1, n_iterations + 1), train_scores, 'b-', label='Training Score')
    plt.xlabel('Training Iterations')
    plt.ylabel('Score')
    plt.title('SGD Learning Curve')
    plt.legend()
    plt.grid(True)
    plt.show()

def plot_sgd_feature_importance(sgd_model, feature_names=None):
    """
    Plot feature importance based on SGD coefficients
    """
    if feature_names is None:
        feature_names = [f'Feature {i}' for i in range(len(sgd_model.coef_[0]))]

    # Get absolute coefficients
    importance = np.abs(sgd_model.coef_[0])

    # Sort features by importance
    sorted_idx = np.argsort(importance)
    pos = np.arange(sorted_idx.shape[0]) + .5

    plt.figure(figsize=(10, 6))
    plt.barh(pos, importance[sorted_idx])
    plt.yticks(pos, np.array(feature_names)[sorted_idx])
    plt.xlabel('|Coefficient Value|')
    plt.title('SGD Feature Importance')
    plt.tight_layout()
    plt.show()

def plot_sgd_decision_regions(sgd_model, X, y, feature_idx=(0,1), h=0.02):
    """
    Plot decision boundaries for 2 features
    """
    # Select two features to plot
    X = X[:, feature_idx]

    # Create mesh grid
    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                         np.arange(y_min, y_max, h))

    # Make predictions
    Z = sgd_model.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)

    # Plot decision boundary
    plt.figure(figsize=(10, 8))
    plt.contourf(xx, yy, Z, alpha=0.4)
    plt.scatter(X[:, 0], X[:, 1], c=y, alpha=0.8)
    plt.xlabel(f'Feature {feature_idx[0]}')
    plt.ylabel(f'Feature {feature_idx[1]}')
    plt.title('SGD Decision Boundaries')
    plt.show()

# Example usage:
# Basic metrics and confusion matrix
plot_sgd_metrics(y_test, y_pred)
plot_sgd_confusion_matrix(y_test, y_pred, sgd_classifier)

# Learning curve (if using partial_fit)
plot_sgd_learning_curve(sgd_classifier, X_train, y_train)

# Feature importance
plot_sgd_feature_importance(sgd_classifier)  # optionally pass feature_names

# Decision boundaries (for 2D visualization)
plot_sgd_decision_regions(sgd_classifier, X, y)





from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Initialize the Random Forest classifier
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)  # You can adjust the number of estimators

# Fit the model on the training data
rf_model.fit(X, y)

# Make predictions on the test set
y_pred_rf = rf_model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred_rf)
accuracy

# Calculate and print the evaluation metrics
print("Accuracy test set:", accuracy_score(y_test, y_pred_rf))
print("Precision test set:", precision_score(y_test, y_pred_rf, average='micro'))
print("Recall test set:", recall_score(y_test, y_pred_rf, average='micro'))
print("F-1 score test set:", f1_score(y_test, y_pred_rf, average='micro'))

import joblib
model_filename = "random_forest_model.joblib"
joblib.dump(rf_model, model_filename)

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

# Compute the confusion matrix
cm = confusion_matrix(y_test, y_pred_rf)

# Set the figure size
plt.figure(figsize=(16, 14))  # Adjust width and height as needed

# Display the confusion matrix
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=rf_model.classes_)
disp.plot(cmap="Blues", values_format="d", ax=plt.gca())

# Show the plot
plt.show()

class_names=['Reconnaissance', 'Exploits', 'DoS', 'Generic', 'Shellcode',
       ' Fuzzers', 'Worms', 'Backdoors', 'Analysis', ' Fuzzers ',
       ' Reconnaissance ', 'Backdoor', ' Shellcode ', 'Reconnaissance ']

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import (accuracy_score, precision_score, recall_score,
                           f1_score, confusion_matrix, ConfusionMatrixDisplay)
from sklearn.tree import plot_tree

def plot_rf_metrics(y_test, y_pred_rf):
    """
    Plot and print performance metrics for Random Forest
    """
    # Calculate metrics
    accuracy = accuracy_score(y_test, y_pred_rf)
    precision = precision_score(y_test, y_pred_rf, average='micro')
    recall = recall_score(y_test, y_pred_rf, average='micro')
    f1 = f1_score(y_test, y_pred_rf, average='micro')

    # Print metrics
    print(f"Random Forest Performance Metrics:")
    print(f"Accuracy: {accuracy:.3f}")
    print(f"Precision: {precision:.3f}")
    print(f"Recall: {recall:.3f}")
    print(f"F1-Score: {f1:.3f}")

    # Create bar plot
    metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']
    values = [accuracy, precision, recall, f1]

    plt.figure(figsize=(10, 6))
    bars = plt.bar(metrics, values)
    plt.ylim(0, 1)
    plt.title('Random Forest Performance Metrics')

    # Add value labels on bars
    for bar in bars:
        height = bar.get_height()
        plt.text(bar.get_x() + bar.get_width()/2., height,
                f'{height:.3f}', ha='center', va='bottom')
    plt.show()

def plot_rf_confusion_matrix(y_test, y_pred_rf, rf_model):
    """
    Plot confusion matrix for Random Forest
    """
    plt.figure(figsize=(10, 8))
    cm = confusion_matrix(y_test, y_pred_rf)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=rf_model.classes_)
    disp.plot(cmap="Blues", values_format="d")
    plt.title('Random Forest Confusion Matrix')
    plt.show()

def plot_feature_importance(rf_model, feature_names=None):
    """
    Plot feature importance from Random Forest
    """
    if feature_names is None:
        feature_names = [f'Feature {i}' for i in range(len(rf_model.feature_importances_))]

    # Get feature importance
    importances = rf_model.feature_importances_
    indices = np.argsort(importances)[::-1]

    plt.figure(figsize=(12, 6))
    plt.title('Random Forest Feature Importances')
    plt.bar(range(len(importances)), importances[indices])
    plt.xticks(range(len(importances)), [feature_names[i] for i in indices], rotation=45, ha='right')
    plt.tight_layout()
    plt.show()

def plot_sample_tree(rf_model, feature_names=None, class_names=None, tree_index=0):
    """
    Plot a single tree from the Random Forest
    """
    plt.figure(figsize=(20,10))
    plot_tree(rf_model.estimators_[tree_index],
             feature_names=feature_names,
             class_names=class_names,
             filled=True,
             rounded=True,
             fontsize=10)
    plt.title(f'Decision Tree #{tree_index} from Random Forest')
    plt.show()

def plot_feature_importance_boxplot(rf_model, feature_names=None):
    """
    Plot feature importance distribution across trees
    """
    if feature_names is None:
        feature_names = [f'Feature {i}' for i in range(rf_model.n_features_in_)]

    importances = []
    for tree in rf_model.estimators_:
        importances.append(tree.feature_importances_)

    importances = np.array(importances)

    plt.figure(figsize=(12, 6))
    plt.boxplot(importances, labels=feature_names)
    plt.xticks(rotation=45, ha='right')
    plt.title('Feature Importance Distribution Across Trees')
    plt.ylabel('Feature Importance')
    plt.tight_layout()
    plt.show()

def plot_tree_depths(rf_model):
    """
    Plot histogram of tree depths in the forest
    """
    depths = [tree.get_depth() for tree in rf_model.estimators_]

    plt.figure(figsize=(10, 6))
    plt.hist(depths, bins='auto', edgecolor='black')
    plt.title('Distribution of Tree Depths in Random Forest')
    plt.xlabel('Tree Depth')
    plt.ylabel('Count')
    plt.show()

# Example usage:

# Basic metrics and confusion matrix
plot_rf_metrics(y_test, y_pred_rf)
plot_rf_confusion_matrix(y_test, y_pred_rf, rf_model)

# Feature importance analysis
plot_feature_importance(rf_model, feature_names)  # overall importance
plot_feature_importance_boxplot(rf_model, feature_names)  # importance distribution

# Tree visualization
plot_sample_tree(rf_model, feature_names, class_names)  # visualize single tree
plot_tree_depths(rf_model)  # analyze tree depths

from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Initialize the Naive Bayes classifier
nb_model = GaussianNB()

# Fit the model on the training data
nb_model.fit(X, y)

# Make predictions on the test set
y_pred_nb = nb_model.predict(X_test)

# Calculate and print the evaluation metrics
print("Accuracy test set:", accuracy_score(y_test, y_pred_nb))
print("Precision test set:", precision_score(y_test, y_pred_nb, average='micro'))
print("Recall test set:", recall_score(y_test, y_pred_nb, average='micro'))
print("F-1 score test set:", f1_score(y_test, y_pred_nb, average='micro'))

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

# Compute the confusion matrix
cm = confusion_matrix(y_test, y_pred_nb)

# Set the figure size
plt.figure(figsize=(16, 14))  # Adjust width and height as needed

# Display the confusion matrix
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=nb_model.classes_)
disp.plot(cmap="Blues", values_format="d", ax=plt.gca())

# Show the plot
plt.show()

models = ['KNeighborsClassifier','Logistic Regression', 'SGDClassifier', 'RandomForestClassifier','Naive Bayes']
test_scores =[]


test_scores.append(accuracy_score(y_test, y_pred_knn))
test_scores.append(accuracy_score(y_test, y_pred_clf))
test_scores.append(accuracy_score(y_test, y_pred))
test_scores.append(accuracy_score(y_test, y_pred_rf))
test_scores.append(accuracy_score(y_test, y_pred_nb))

plt.figure(figsize=(10, 12))
plt.title('Accuracies of Different Сlassifer Models')
plt.bar(models, test_scores)
plt.xlabel('Сlassifer Models')
plt.ylabel('Testing Accuracy')

plt.show()

pip install shap

import shap

rf_model = RandomForestClassifier(n_estimators=100, random_state=42)

rf_model.fit(X, y)

y_pred_rf = rf_model.predict(X_test)

print("Accuracy test set:", accuracy_score(y_test, y_pred_rf))
print("Precision test set:", precision_score(y_test, y_pred_rf, average='micro'))
print("Recall test set:", recall_score(y_test, y_pred_rf, average='micro'))
print("F-1 score test set:", f1_score(y_test, y_pred_rf, average='micro'))

rf_model.fit(X, y)  # Ensure model is fitted if not already done

# Initialize SHAP TreeExplainer
explainer = shap.TreeExplainer(rf_model)  # Optimized for tree-based models

# Select the first 100 observations
X_test_sample = X_test.iloc[:10000]  # Slicing first 10000 rows

# Compute SHAP values for the sample
shap_values = explainer.shap_values(X_test_sample)

# Plot the SHAP summary for these observations
shap.summary_plot(shap_values[1], X_test_sample, feature_names=X.columns)

shap.initjs()

# Initialize TreeExplainer
explainer = shap.TreeExplainer(rf_model)

# Select the first 10,000 rows
X_test_sample = X_test.iloc[:10000]  # Adjust if X_test has fewer than 10,000 rows

# Compute SHAP values for the sample
shap_values = explainer.shap_values(X_test_sample)

# Summary plot for feature importance
shap.summary_plot(shap_values[1], X_test_sample, feature_names=X.columns)

"""# Application"""

pip install gradio pillow<9.0.0

import sklearn
print(sklearn.__version__)

import gradio as gr
import joblib
import numpy as np
from pathlib import Path

# Load models and encoders
models_path = Path(".")
model = joblib.load(models_path / "random_forest_model.joblib")
le_protocol = joblib.load(models_path / "protocol_encoder.joblib")
le_attack_subcategory = joblib.load(models_path / "attack_subcategory_encoder.joblib")

# Get valid protocols and attack subcategories
VALID_PROTOCOLS = sorted(le_protocol.classes_)
VALID_ATTACK_SUBCATEGORIES = sorted(le_attack_subcategory.classes_)

def predict_attack(protocol: str, source_port: int, destination_port: int, attack_subcategory: str) -> str:
    try:
        if not (0 <= source_port <= 65535 and 0 <= destination_port <= 65535):
            return "Port numbers must be between 0 and 65535"

        protocol_encoded = le_protocol.transform([protocol])[0]
        attack_subcategory_encoded = le_attack_subcategory.transform([attack_subcategory])[0]

        input_data = np.array([[protocol_encoded, source_port, destination_port, attack_subcategory_encoded]])
        prediction = model.predict(input_data)[0]
        return f"Predicted attack category: {prediction}"

    except Exception as e:
        return f"Error: {str(e)}"

demo = gr.Interface(
    fn=predict_attack,
    inputs=[
        gr.Dropdown(choices=VALID_PROTOCOLS, label="Protocol"),
        gr.Number(label="Source Port", minimum=0, maximum=65535),
        gr.Number(label="Destination Port", minimum=0, maximum=65535),
        gr.Dropdown(choices=VALID_ATTACK_SUBCATEGORIES, label="Attack Subcategory"),
    ],
    outputs=gr.Textbox(label="Prediction"),
    title="Network Attack Classifier",
    description="Select network traffic parameters to predict the attack category."
)

if __name__ == "__main__":
    demo.launch()